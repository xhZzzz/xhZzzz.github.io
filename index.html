<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>界王星 - 界王星</title><meta name="keywords" content="知识图谱，深度学习"><meta name="author" content="Xu Hao"><meta name="copyright" content="Xu Hao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="界王星">
<meta property="og:url" content="http://xhzzzz.github.io/index.html">
<meta property="og:site_name" content="界王星">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img2.baidu.com/it/u=2642132708,127949625&fm=26&fmt=auto">
<meta property="article:author" content="Xu Hao">
<meta property="article:tag" content="知识图谱，深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img2.baidu.com/it/u=2642132708,127949625&fm=26&fmt=auto"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://xhzzzz.github.io/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":50},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Xu Hao","link":"链接: ","source":"来源: 界王星","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '界王星',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2021-10-31 20:05:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://img2.baidu.com/it/u=2642132708,127949625&amp;fm=26&amp;fmt=auto" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('https://images4.alphacoders.com/714/thumbbig-714675.webp')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">界王星</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">界王星</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/xhZzzz" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:xxxxxx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/10/31/NLP%E5%B8%B8%E7%94%A8metrics%E6%95%B4%E7%90%86/" title="NLP常用metrics整理">     <img class="post_bg" src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NLP常用metrics整理"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/10/31/NLP%E5%B8%B8%E7%94%A8metrics%E6%95%B4%E7%90%86/" title="NLP常用metrics整理">NLP常用metrics整理</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-10-31T02:47:06.000Z" title="发表于 2021-10-31 10:47:06">2021-10-31</time></span></div><div class="content">最近阅读论文发现了几个不同的评价指标，本文整理一下NLP中生成问题的评估指标
nlp任务中的生成问题大致可分为：

语言模型；
文本翻译；
内容概括和扩写；
。。。

我们需要对一段机器生成的文本来评估其生成质量的好坏。这里又可以分为有监督和无监督的，有监督的有一些参考的生成结果可以比对；无监督的没有参考文本，只能通过文本本身来评估生成质量。细分如下：

无监督（通过文本本身来评估）
PPL（困惑度指标）


有监督（有已生成的参考文本对比）
纯字符匹配方式（优点在于速度快，缺点在于无法真正理解语义，因此和真实的质量评估上总是存在一个不小的gap）
BLEU
ROUGE


模型指标学习（在文本评估方面更加准确，但是需要大量数据，运行起来速度慢）
bleurt


人工评判方式



ppl(perplexity)表示一句话中每一个词出现的频率倒数的均方平均，公式表示：
ppl=\sqrt[n]{\prod_{w_i}\frac{1}{p(w_i|w_0,w_1,...,w_{i-1})}}

通常来讲，ppl越小，表示上下文连贯度越高，文本越通顺

但是由于n-gram的概率计算复杂 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/10/29/Transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="Transformer论文阅读笔记">     <img class="post_bg" src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer论文阅读笔记"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/10/29/Transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="Transformer论文阅读笔记">Transformer论文阅读笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-10-29T15:15:14.000Z" title="发表于 2021-10-29 23:15:14">2021-10-29</time></span></div><div class="content">论文：Attention Is All You Need
摘要核心
常用的主流序列模型都是基于卷积神经网络或者循环神经网络，表现最好的模型也是基于encoder-decoder框架的基础上加上attention机制
提出一种完全依靠attention机制的新模型transformer，放弃了卷积神经网络和循环网络
在2014WMT翻译数据集上，比现存最好的模型的bleu值高2个点

Introduction
Recurrent模型通常沿着输入输出的位置信息进行计算，将位置和计算时间步骤联系起来，产生一个隐藏状态$ht$，作为前一个隐藏状态$h{(t-1)}$和位置$t$输入的函数。这种固定的顺序阻止了训练实例的并行化，在较长序列中变得尤其关键，因为内存限制了跨实例的批处理。
注意力机制不需要依靠他们在输入输出序列中的距离就能够对其依赖关系进行建模
提出了transformer模型，仅仅只依靠注意力机制来描绘输入输出之间的全局依赖关系

Background

主要是self-attention机制，来计算序列不同位置之间相关性的表征

Model Architecture
encode ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/10/28/Building-a-Large-scale-Multimodal-Knowledge-Base-System-for-Answering-Visual-Queries/" title="Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries">     <img class="post_bg" src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/10/28/Building-a-Large-scale-Multimodal-Knowledge-Base-System-for-Answering-Visual-Queries/" title="Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries">Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-10-28T13:41:22.000Z" title="发表于 2021-10-28 21:41:22">2021-10-28</time></span></div><div class="content">论文：Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries 
创新点：

把MRF应用于知识库表示中，这样可以结合连续的图像特征变量和其它的离散变量。
提出了一种声明式语言用于辅助检索
采用了factor graph ，能够高效表示实体间的关系

摘要
提出了一个知识库框架处理视觉查询任务，在遇到新的任务时不需要重新训练分类器
把大规模MRF（马尔科夫条件随机场）用于KB表示，结合视觉、文本、其他结构性数据和它们之间的关系

Introduction现有的基于NLP和视觉的查询任务都存在面对复杂问题查询时不能够识别的问题，一些文章提出将视觉识别任务放入一个异质性推理和推断更强的框架之中—————这样做的好处是在每次新的类型任务出现时不需要重新训练分类器
作者受NEIL模型和Markov Logic模型的启发，且侧重解决它们存在的问题，提出了自己的模型：

NEIL模型的缺点：在识别任务中测试场景受限，同时还缺乏一个配套的推理模型在不需要训练新的分类器的条件下扩展到更丰富的 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/10/13/GloVe%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="GloVe论文阅读笔记">     <img class="post_bg" src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GloVe论文阅读笔记"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/10/13/GloVe%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="GloVe论文阅读笔记">GloVe论文阅读笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-10-13T08:00:00.000Z" title="发表于 2021-10-13 16:00:00">2021-10-13</time></span></div><div class="content">论文：GloVe: Global Vectors for Word Representation
GloVe模型分析与其他模型的比较1、与基于矩阵分解(Matrix Factorization Methods)的方法比较

缺点：在词对推理任务上表现特别差

2、基于上下文的向量学习方法(Shallow Window-Based Methods)典型模型：Word2Vector（CBOW和Skip-gram）

缺点：无法使用全局的统计信息，只是用了上下文的局部信息（作者的观点）
实际上对出现次数不同的词优化次数不一样，通过优化的次数多少在一定程度上也能体现一定的全局信息


论文摘要：
当前词向量学习模型能够通过向量间的算术运算捕捉词之间细微的语法和语义规律”$V(King)-V(Queen)=V(Man)-V(Woman)$”
通过仔细分析我们发现了一些有利于这种词向量规律的特性，并基于词提出了一种新的对数双线性回归模型(global log-bilinear regression model)。这种模型能够利用全局矩阵分解和局部上下文的优点来学习词向量
我们的模型只在共现矩阵中非 ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/10/07/Softmax%E5%92%8CSigmoid%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/" title="Softmax和Sigmoid笔记总结">     <img class="post_bg" src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Softmax和Sigmoid笔记总结"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/10/07/Softmax%E5%92%8CSigmoid%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/" title="Softmax和Sigmoid笔记总结">Softmax和Sigmoid笔记总结</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-10-07T04:54:00.000Z" title="发表于 2021-10-07 12:54:00">2021-10-07</time></span></div><div class="content">1、Softmax function
g(x)=log\sum_{i=1}^ne^{x_i}
可见softmax是从一个输入序列中算出一个值

由于$e^{x{max}}&gt;&gt;e^{x_i}$,所以$g(x)=log\sum{i=1}^ne^{xi}\approx loge^{x{max}}=x_{max}$

softmax会返回输入序列中最大的那个值得近似值。即softmax是对真max函数的近似，softmax的函数曲线是光滑的（处处可微分），而max(0,x)之类的函数会有折点


在分类算法中需要从一组可能的结果中找出概率最大的那个，需要用到max函数。但是为了可以优化，描述函数必须要求是可微分的，这样用softmax代替max就是一个非常合适的选择

2、Softmax activation function(也叫归一化指数数函)
f(x_i)=\frac{e^{x_i}}{\sum_{i=0}^ke^{x_i}}
softmax activation function返回的是一个序列，分母雷士softmax function中的一部分，在效果上：通过运算会扩大最 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/" title="NLPBaseline系列之Word2Vec模型">     <img class="post_bg" src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NLPBaseline系列之Word2Vec模型"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/" title="NLPBaseline系列之Word2Vec模型">NLPBaseline系列之Word2Vec模型</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-09-29T12:00:00.000Z" title="发表于 2021-09-29 20:00:00">2021-09-29</time></span></div><div class="content">相关论文:

Efficient Estimation of Word Representations in Vector Space
Distributed Representations of Words and Phrases and their Compositionality
A Neural Probabilistic Language Model  

语言模型(Language Model—-“LM”)  

概念：语言模型是计算一个句子是句子的概率Example： 
今天下雨了—————0.9
烬天瞎与乐—————0.001
净添吓玉勒—————0.00005


类别
基于专家语法规则的语言模型
统计语言模型



统计语言模型：通过概率来刻画语言模型  
𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤1)P(𝑤2|𝑤1)P(𝑤3|𝑤1𝑤2)…P(𝑤𝑛|𝑤1𝑤2 … 𝑤𝑛−1)如何求解$P(w_i)$?

用语料的频率代替概率（频率学派、条件学派） 

P(w_i)=\frac{count(w_i)}{N}


语 ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/09/28/CS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Lecture%2001%20Introduce%20and%20Word%20Vectors/" title="Lecture 01 Introduction and Word Vectors">     <img class="post_bg" src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Lecture 01 Introduction and Word Vectors"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/28/CS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Lecture%2001%20Introduce%20and%20Word%20Vectors/" title="Lecture 01 Introduction and Word Vectors">Lecture 01 Introduction and Word Vectors</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-09-28T15:00:00.000Z" title="发表于 2021-09-28 23:00:00">2021-09-28</time></span></div><div class="content">Human language and word meaning人类的语言具有信息功能和社会功能
How do we represent the meaning of a word?meaning

用一个词、词组来表示概念
一个人想用语言、符号等来表达的想法
表达作品思想How do we have usable meaning in a computer?WordNet, 一个包含同义词集和上位词(“is a”关系) synonym sets and hypernyms 的列表的辞典Problems with resources like WordNet  
作为一个资源很好，但忽略了细微差别 
例如“proficient”被列为“good”的同义词。这只在某些上下文中是正确的。


缺少单词的新含义
难以持续更新
例如 wicked, badass, nifty, wizard, genius, ninja, bombest


主观的
需要人类劳动来创造和调整
无法计算单词相似度  

基于符号的表示方法传统的自然语言处理中，我们把词语看做离散的符号。通过one-hot编码对词汇 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/09/27/hello-world/" title="Hello World">     <img class="post_bg" src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/27/hello-world/" title="Hello World">Hello World</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-09-27T12:53:51.228Z" title="发表于 2021-09-27 20:53:51">2021-09-27</time></span></div><div class="content">Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.
Quick StartCreate a new post$ hexo new &quot;My New Post&quot;
More info: Writing
Run server$ hexo server
More info: Server
Generate static files$ hexo generate
More info: Generating
Deploy to remote sites$ hexo deploy
More info: Deployment
</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://img2.baidu.com/it/u=2642132708,127949625&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Xu Hao</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xhZzzz" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:xxxxxx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/10/31/NLP%E5%B8%B8%E7%94%A8metrics%E6%95%B4%E7%90%86/" title="NLP常用metrics整理"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NLP常用metrics整理"/></a><div class="content"><a class="title" href="/2021/10/31/NLP%E5%B8%B8%E7%94%A8metrics%E6%95%B4%E7%90%86/" title="NLP常用metrics整理">NLP常用metrics整理</a><time datetime="2021-10-31T02:47:06.000Z" title="发表于 2021-10-31 10:47:06">2021-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/29/Transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="Transformer论文阅读笔记"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer论文阅读笔记"/></a><div class="content"><a class="title" href="/2021/10/29/Transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="Transformer论文阅读笔记">Transformer论文阅读笔记</a><time datetime="2021-10-29T15:15:14.000Z" title="发表于 2021-10-29 23:15:14">2021-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/28/Building-a-Large-scale-Multimodal-Knowledge-Base-System-for-Answering-Visual-Queries/" title="Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries"/></a><div class="content"><a class="title" href="/2021/10/28/Building-a-Large-scale-Multimodal-Knowledge-Base-System-for-Answering-Visual-Queries/" title="Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries">Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries</a><time datetime="2021-10-28T13:41:22.000Z" title="发表于 2021-10-28 21:41:22">2021-10-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/13/GloVe%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="GloVe论文阅读笔记"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GloVe论文阅读笔记"/></a><div class="content"><a class="title" href="/2021/10/13/GloVe%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="GloVe论文阅读笔记">GloVe论文阅读笔记</a><time datetime="2021-10-13T08:00:00.000Z" title="发表于 2021-10-13 16:00:00">2021-10-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/07/Softmax%E5%92%8CSigmoid%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/" title="Softmax和Sigmoid笔记总结"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Softmax和Sigmoid笔记总结"/></a><div class="content"><a class="title" href="/2021/10/07/Softmax%E5%92%8CSigmoid%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/" title="Softmax和Sigmoid笔记总结">Softmax和Sigmoid笔记总结</a><time datetime="2021-10-07T04:54:00.000Z" title="发表于 2021-10-07 12:54:00">2021-10-07</time></div></div></div></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/NLP/" style="font-size: 1.1em; color: #999">NLP</a> <a href="/tags/NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" style="font-size: 1.1em; color: #999">NLP预训练模型</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/10/"><span class="card-archive-list-date">十月 2021</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/09/"><span class="card-archive-list-date">九月 2021</span><span class="card-archive-list-count">3</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">8</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2021-10-31T12:05:55.014Z"></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Xu Hao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
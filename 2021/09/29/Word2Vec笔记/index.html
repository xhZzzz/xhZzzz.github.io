<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>NLPBaseline系列之Word2Vec模型 | 界王星</title><meta name="keywords" content="博客,深度学习"><meta name="author" content="Xu Hao"><meta name="copyright" content="Xu Hao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="相关论文:  Efficient Estimation of Word Representations in Vector Space Distributed Representations of Words and Phrases and their Compositionality A Neural Probabilistic Language Model    语言模型(Language M">
<meta property="og:type" content="article">
<meta property="og:title" content="NLPBaseline系列之Word2Vec模型">
<meta property="og:url" content="http://xhzzzz.github.io/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="界王星">
<meta property="og:description" content="相关论文:  Efficient Estimation of Word Representations in Vector Space Distributed Representations of Words and Phrases and their Compositionality A Neural Probabilistic Language Model    语言模型(Language M">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img2.baidu.com/it/u=196582304,1580603520&fm=26&fmt=auto">
<meta property="article:published_time" content="2021-09-29T12:30:03.166Z">
<meta property="article:modified_time" content="2021-09-30T02:50:41.699Z">
<meta property="article:author" content="Xu Hao">
<meta property="article:tag" content="博客,深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img2.baidu.com/it/u=196582304,1580603520&fm=26&fmt=auto"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://xhzzzz.github.io/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":50},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Xu Hao","link":"链接: ","source":"来源: 界王星","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLPBaseline系列之Word2Vec模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-09-30 10:50:41'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://img2.baidu.com/it/u=2642132708,127949625&amp;fm=26&amp;fmt=auto" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">3</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">界王星</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NLPBaseline系列之Word2Vec模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-29T12:30:03.166Z" title="发表于 2021-09-29 20:30:03">2021-09-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-30T02:50:41.699Z" title="更新于 2021-09-30 10:50:41">2021-09-30</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="NLPBaseline系列之Word2Vec模型"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>相关论文:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a></li>
<li><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed Representations of Words and Phrases and their Compositionality</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/volume3/tmp/bengio03a.pdf">A Neural Probabilistic Language Model</a>  </li>
</ul>
<p><strong>语言模型(Language Model—“LM”)</strong>  </p>
<ul>
<li>概念：语言模型是计算一个句子是句子的概率<br>Example： <ul>
<li>今天下雨了———-0.9</li>
<li>烬天瞎与乐———-0.001</li>
<li>净添吓玉勒———-0.00005</li>
</ul>
</li>
<li>类别<ul>
<li>基于专家语法规则的语言模型</li>
<li><strong>统计语言模型</strong></li>
</ul>
</li>
</ul>
<p><strong>统计语言模型</strong>：通过概率来刻画语言模型<br>$$𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤1)P(𝑤2|𝑤1)P(𝑤3|𝑤1𝑤2)…P(𝑤𝑛|𝑤1𝑤2 … 𝑤𝑛−1)$$</p>
<p>如何求解$P(w_i)$?</p>
<blockquote>
<p>用语料的频率代替概率（频率学派、条件学派） </p>
<blockquote>
<p>$$P(w_i)=\frac{count(w_i)}{N}$$  </p>
</blockquote>
</blockquote>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929214832155.png" alt="image-20210929214832155"></p>
<p><strong>语言模型中存在的一些问题</strong></p>
<ul>
<li>当句子中出现语料库中没有出现的词时，会导致句子的概率为0，但是这不意味着这个词（句子）就不会出现在未来的句子中（<em>仅仅只是因为我们的语料库不够充分导致我们误判了该句子出现的概率为0</em>）</li>
<li>当短语过长也可能会导致该短语的出现的概率为0（短语越长越有可能出现这种状况），但是这种短语在未来还是有可能出现的，不应该直接判断为0</li>
</ul>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929225102402.png" alt="image-20210929225102402"></p>
<p><strong>解决办法</strong>：引入平滑操作</p>
<h1 id="统计语言模型中的平滑操作"><a href="#统计语言模型中的平滑操作" class="headerlink" title="统计语言模型中的平滑操作"></a>统计语言模型中的平滑操作</h1><p>有一些词或词组在语料中没有出现过，但是不代表它不可能存在。平滑操作就是给那些没出现过的词或词组一个比较小的概率</p>
<h2 id="1、Laplace-Smoothing-也称Add-one-Smoothing"><a href="#1、Laplace-Smoothing-也称Add-one-Smoothing" class="headerlink" title="1、Laplace Smoothing  也称Add-one Smoothing"></a>1、Laplace Smoothing  也称Add-one Smoothing</h2><blockquote>
<p>Origine: $P(w)=\frac{C(w)}{N}$</p>
<p>Laplace Smoothing:$P(w)=\frac{C(w)+1}{N+V}$</p>
</blockquote>
<p>$C(w)$:表示w出现的次数。<br>$N$:表示词库的维度大小<br>$V$:表示句子中词的个数</p>
<h2 id="2、Add-K-Smoothing"><a href="#2、Add-K-Smoothing" class="headerlink" title="2、Add-K Smoothing"></a>2、Add-K Smoothing</h2><blockquote>
<p>Origine: $P(w)=\frac{C(w)}{N}$</p>
</blockquote>
<blockquote>
<p>Laplace Smoothing:$P(w)=\frac{C(w)+K}{N+K*V}$</p>
</blockquote>
<h2 id="马尔科夫假设"><a href="#马尔科夫假设" class="headerlink" title="马尔科夫假设"></a>马尔科夫假设</h2><p>下一个词的出现仅依赖于前面的一个词或几个词  </p>
<p>$$𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤<em>1)P(𝑤_2|𝑤_1)P(𝑤_3|𝑤_1𝑤_2)…P(𝑤_𝑛|𝑤_1𝑤_2 … 𝑤</em>{𝑛−1})$$</p>
<p><strong>unigram</strong>:$𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤_1)P(𝑤_2)P(𝑤_3)…P(𝑤_𝑛)$</p>
<p><strong>bigram</strong>:$𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤<em>1)P(𝑤_2|𝑤_1)P(𝑤_3|𝑤_2)…P(𝑤_𝑛|𝑤</em>{𝑛−1})$</p>
<p><strong>trigram</strong>:$𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤<em>1)P(𝑤_2|𝑤_1)P(𝑤_3|𝑤_1𝑤_2)…P(𝑤𝑛|𝑤</em>{n-3}𝑤_{n-2}𝑤_{n-1})$</p>
<p><strong>K-gram</strong>:$𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤<em>1)P(𝑤_2|𝑤_1)P(𝑤_3|𝑤_1𝑤_2)…P(𝑤𝑛|𝑤</em>{n-k}…𝑤_{n-1})$</p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929231331739.png" alt="image-20210929231331739"></p>
<h1 id="语言模型的评价指标：困惑度-Perplexity"><a href="#语言模型的评价指标：困惑度-Perplexity" class="headerlink" title="语言模型的评价指标：困惑度(Perplexity)"></a>语言模型的评价指标：困惑度(Perplexity)</h1><p>$$𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤<em>1)P(𝑤_2|𝑤_1)P(𝑤_3|𝑤_1𝑤_2)…P(𝑤_𝑛|𝑤_1𝑤_2 … 𝑤</em>{𝑛−1})$$</p>
<p>$$PP(s)=P(w_1,w_2,…,w_n)^{-\frac{1}{n}}=\sqrt[N]{\frac{1}{P(w_1,w_2,…,w_n)}}$$</p>
<blockquote>
<p>句子概率$P(s)$越大,语言模型越好，困惑度$PP(s)$越小</p>
</blockquote>
<h1 id="词的表示方法"><a href="#词的表示方法" class="headerlink" title="词的表示方法"></a>词的表示方法</h1><h2 id="One-hot编码（独热编码）"><a href="#One-hot编码（独热编码）" class="headerlink" title="One-hot编码（独热编码）"></a>One-hot编码（独热编码）</h2><ul>
<li>优点：表示简单</li>
<li>缺点：<ul>
<li>编码的维度等于词的数量，词越多维度越高，且向量非常稀疏</li>
<li>词语词之间的语义信息无法表示</li>
</ul>
</li>
</ul>
<blockquote>
<p>Example:</p>
<ul>
<li>“话筒”表示为[0 0 0 1 0 0 0 …]</li>
<li>“麦克风”表示为[0 0 0 0 0 1 0 …]</li>
</ul>
<p>$V(话筒)*V(麦克风)=0$我们希望通过向量的点乘表示出向量之间的一定的关系，话筒和麦克风之间显然有一定关系，但是one-hot编码后无法表征出来</p>
</blockquote>
<blockquote>
<p>稀疏矩阵的表示方法:</p>
<ul>
<li>“话筒”  3:1</li>
<li>“麦克风”  5:1</li>
</ul>
</blockquote>
<h2 id="共生矩阵表示"><a href="#共生矩阵表示" class="headerlink" title="共生矩阵表示"></a>共生矩阵表示</h2><p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929233232531.png" alt="image-20210929233232531"></p>
<h2 id="分布式表示-稠密表示-Distributed-Representation"><a href="#分布式表示-稠密表示-Distributed-Representation" class="headerlink" title="分布式表示/稠密表示(Distributed Representation)"></a>分布式表示/稠密表示(Distributed Representation)</h2><p>分布式表示的概念是深度学习的核心，特别是在它用于NLP任务时。</p>
<p><strong>1、稀疏/本地化的非分布式形状表示（one-hot）</strong></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929234500604.png" alt="&quot;稀疏或叫本地化的非分布式的形状的表示&quot;" title="稀疏或叫本地化的非分布式的形状的表示"></p>
<p>这种非分布式的表示方法在很多方面是效率低下的：</p>
<ul>
<li>随着观察的图形的数量增加，表示的维度增加</li>
<li>没有体现图形之间的相识度</li>
</ul>
<p><strong>2、分布式表示</strong></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929234945667.png" alt="image-20210929234945667"></p>
<p>它用与方向和形状概念相关的多个“记忆单元”表示形状的信息。这些“记忆单元”含有各个形状以及形状和形状之间的关联信息。当我们用分布式表示方法表示一个新的形状时就不用再添加新的维度。</p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929235402278.png" alt="image-20210929235402278"></p>
<h1 id="对比模型"><a href="#对比模型" class="headerlink" title="对比模型"></a>对比模型</h1><ul>
<li>前馈神经网络语言模型(NNLM)</li>
<li>循环神经网络语言模型(RNNLM)</li>
</ul>
<blockquote>
<p>语言模型是无监督的（不需要标注语料）—用周围词或之前的词就可以得到词的概率生成样本训练</p>
<p>我们目标是通过无监督的方法进行监督学习</p>
</blockquote>
<p><strong>1、NNLM</strong></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930000326605.png" alt="image-20210930000326605"></p>
<blockquote>
<p>根据前n-1个单词预测第n个位子单词的概率</p>
<p>优化模型，使得输出的正确的单词概率最大化</p>
</blockquote>
<ul>
<li>输入层：将词映射成向量，相当于一个$1<em>V$的one-hot向量乘以一个$V</em>D$的向量得到一个$1*D$的向量</li>
</ul>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930001207016.png" alt="image-20210930001207016"></p>
<blockquote>
<p>相当于用one-hot编码向量来查找自己的词向量(本图例子即为$(w_{41},w_{42},w_{43})$)</p>
<p>词向量矩阵$V<em>D$其中$D$是特征数，我们自己设置的*<em>超参数</em></em></p>
</blockquote>
<ul>
<li><p>隐藏层：一个以tanh为激活函数的全连接层</p>
<ul>
<li>$$a=tanh(d+Ux)$$</li>
</ul>
</li>
<li><p>输出层：一个全连接层，后面接一个softmax函数来生成概率分布</p>
<ul>
<li>$$y=b+Wa$$</li>
<li>其中y是一个$1*V$的向量：$P(w_t|w_{t-n},…,w_{t-1})-\frac{exp(y_{w_t})}{\sum_i{exp(y_i)}})$</li>
</ul>
</li>
<li><p>Perplexity和Loss</p>
<ul>
<li>$$Loss:L=-\frac{1}{T}\sum_{i=1}^{T}logp(w_i|w_{i-n+1},…,w_{i-1})$$</li>
<li>$$PP(s)=P(w_1,w_2,…,w_T)^{-\frac{1}{T}}=\sqrt[T]{\frac{1}{P(w_1,w_2,…,w_T)}}$$</li>
<li>$$log(PP(s))=-\frac{1}{T}log(P(w_1)P(w_2|w_1),…,P(w_T|w_{T-n+1},…,w_{T-1}))$$<ul>
<li>$$log(PP(s))=-\frac{1}{T}(logP(w_1)+logP(w_2|w_1)+…+P(w_T|w_{T-n+1},…,w_{T-1}))$$</li>
<li>$$PP(s)=e^L$$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>2、RNNLM</strong></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930101740093.png" alt="image-20210930101740093"></p>
<hr>
<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p><strong>语言模型思想</strong>：句子中下一个词的出现和前面词是有关系的，所以可以用<strong>前面的词</strong>预测<strong>下一个词</strong>。</p>
<p><strong>Word2Vec思想</strong>：句子中<strong>相近的词</strong>之间是有关系的，Word2Vec的基本思想就是用词来预测词。其中，<strong>Skip-gram</strong>使用中心词预测周围词，<strong>CBOW</strong>使用周围词预测中心词。</p>
<h2 id="Skip-gram-跳字模型"><a href="#Skip-gram-跳字模型" class="headerlink" title="Skip-gram(跳字模型)"></a>Skip-gram(跳字模型)</h2><p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930102330855.png" alt="image-20210930102330855"></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930102714863.png" alt="image-20210930102714863"></p>
<blockquote>
<p>1、将中心词(的index)映射成向量</p>
<p>2、中心词向量乘以<strong>中心词的词向量矩阵</strong>得到特征向量</p>
<p>3、得到的特征向量乘以<strong>周围词的词向量矩阵</strong>得到输出向量</p>
<p>4、输出向量经过softmax函数可以得到周围词的概率分布</p>
</blockquote>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930103303959.png" alt="image-20210930103303959"></p>
<h2 id="CBOW-词袋模型"><a href="#CBOW-词袋模型" class="headerlink" title="CBOW(词袋模型)"></a>CBOW(词袋模型)</h2><p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930103645235.png" alt="image-20210930103645235"></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930103714231.png" alt="image-20210930103714231"></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930103923188.png" alt="image-20210930103923188"></p>
<h2 id="加速softmax"><a href="#加速softmax" class="headerlink" title="加速softmax"></a>加速softmax</h2><h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><ul>
<li>思想：把求softmax转化成求几个sigmoid</li>
</ul>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930104300923.png" alt="image-20210930104300923"></p>
<blockquote>
<p>层次softmax只需要$log_2V$个sigmoid————————-（V–&gt;$log_2V$）</p>
</blockquote>
<p><strong>Huffman树的构建</strong>：带权重路径最短二叉树</p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930104758929.png" alt="image-20210930104758929"></p>
<p><strong>Skip-gram的层次softmax构建</strong></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930105038471.png" alt="image-20210930105038471"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Xu Hao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://xhzzzz.github.io/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/">http://xhzzzz.github.io/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://xhZzzz.github.io" target="_blank">界王星</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/09/28/CS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Lecture%2001%20Introduce%20and%20Word%20Vectors/"><img class="next-cover" src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Lecture 01 Introduction and Word Vectors</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://img2.baidu.com/it/u=2642132708,127949625&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Xu Hao</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">3</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xhZzzz" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:xxxxxx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%B3%E6%BB%91%E6%93%8D%E4%BD%9C"><span class="toc-text">统计语言模型中的平滑操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81Laplace-Smoothing-%E4%B9%9F%E7%A7%B0Add-one-Smoothing"><span class="toc-text">1、Laplace Smoothing  也称Add-one Smoothing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81Add-K-Smoothing"><span class="toc-text">2、Add-K Smoothing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%81%87%E8%AE%BE"><span class="toc-text">马尔科夫假设</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%EF%BC%9A%E5%9B%B0%E6%83%91%E5%BA%A6-Perplexity"><span class="toc-text">语言模型的评价指标：困惑度(Perplexity)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%8D%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95"><span class="toc-text">词的表示方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#One-hot%E7%BC%96%E7%A0%81%EF%BC%88%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81%EF%BC%89"><span class="toc-text">One-hot编码（独热编码）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B1%E7%94%9F%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA"><span class="toc-text">共生矩阵表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA-%E7%A8%A0%E5%AF%86%E8%A1%A8%E7%A4%BA-Distributed-Representation"><span class="toc-text">分布式表示&#x2F;稠密表示(Distributed Representation)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E6%A8%A1%E5%9E%8B"><span class="toc-text">对比模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Word2Vec"><span class="toc-text">Word2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Skip-gram-%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B"><span class="toc-text">Skip-gram(跳字模型)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CBOW-%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">CBOW(词袋模型)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A0%E9%80%9Fsoftmax"><span class="toc-text">加速softmax</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hierarchical-Softmax"><span class="toc-text">Hierarchical Softmax</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/" title="NLPBaseline系列之Word2Vec模型"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NLPBaseline系列之Word2Vec模型"/></a><div class="content"><a class="title" href="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/" title="NLPBaseline系列之Word2Vec模型">NLPBaseline系列之Word2Vec模型</a><time datetime="2021-09-29T12:30:03.166Z" title="发表于 2021-09-29 20:30:03">2021-09-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/28/CS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Lecture%2001%20Introduce%20and%20Word%20Vectors/" title="Lecture 01 Introduction and Word Vectors"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Lecture 01 Introduction and Word Vectors"/></a><div class="content"><a class="title" href="/2021/09/28/CS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Lecture%2001%20Introduce%20and%20Word%20Vectors/" title="Lecture 01 Introduction and Word Vectors">Lecture 01 Introduction and Word Vectors</a><time datetime="2021-09-28T15:26:57.236Z" title="发表于 2021-09-28 23:26:57">2021-09-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/27/hello-world/" title="Hello World"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/2021/09/27/hello-world/" title="Hello World">Hello World</a><time datetime="2021-09-27T12:53:51.228Z" title="发表于 2021-09-27 20:53:51">2021-09-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Xu Hao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
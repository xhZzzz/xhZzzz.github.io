<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>NLPBaseline系列之Word2Vec模型 | 界王星</title><meta name="keywords" content="知识图谱，深度学习"><meta name="author" content="Xu Hao"><meta name="copyright" content="Xu Hao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="相关论文:  Efficient Estimation of Word Representations in Vector Space Distributed Representations of Words and Phrases and their Compositionality A Neural Probabilistic Language Model    语言模型(Language M">
<meta property="og:type" content="article">
<meta property="og:title" content="NLPBaseline系列之Word2Vec模型">
<meta property="og:url" content="http://xhzzzz.github.io/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="界王星">
<meta property="og:description" content="相关论文:  Efficient Estimation of Word Representations in Vector Space Distributed Representations of Words and Phrases and their Compositionality A Neural Probabilistic Language Model    语言模型(Language M">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img2.baidu.com/it/u=196582304,1580603520&fm=26&fmt=auto">
<meta property="article:published_time" content="2021-09-29T12:00:00.000Z">
<meta property="article:modified_time" content="2021-10-08T02:54:12.481Z">
<meta property="article:author" content="Xu Hao">
<meta property="article:tag" content="知识图谱，深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img2.baidu.com/it/u=196582304,1580603520&fm=26&fmt=auto"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://xhzzzz.github.io/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":50},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Xu Hao","link":"链接: ","source":"来源: 界王星","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLPBaseline系列之Word2Vec模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-10-08 10:54:12'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://img2.baidu.com/it/u=2642132708,127949625&amp;fm=26&amp;fmt=auto" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">界王星</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NLPBaseline系列之Word2Vec模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-29T12:00:00.000Z" title="发表于 2021-09-29 20:00:00">2021-09-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-08T02:54:12.481Z" title="更新于 2021-10-08 10:54:12">2021-10-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="NLPBaseline系列之Word2Vec模型"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>相关论文:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a></li>
<li><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed Representations of Words and Phrases and their Compositionality</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/volume3/tmp/bengio03a.pdf">A Neural Probabilistic Language Model</a>  </li>
</ul>
<p><strong>语言模型(Language Model—-“LM”)</strong>  </p>
<ul>
<li>概念：语言模型是计算一个句子是句子的概率<br>Example： <ul>
<li>今天下雨了—————0.9</li>
<li>烬天瞎与乐—————0.001</li>
<li>净添吓玉勒—————0.00005</li>
</ul>
</li>
<li>类别<ul>
<li>基于专家语法规则的语言模型</li>
<li><strong>统计语言模型</strong></li>
</ul>
</li>
</ul>
<p><strong>统计语言模型</strong>：通过概率来刻画语言模型  </p>
<script type="math/tex; mode=display">𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤1)P(𝑤2|𝑤1)P(𝑤3|𝑤1𝑤2)…P(𝑤𝑛|𝑤1𝑤2 … 𝑤𝑛−1)</script><p>如何求解$P(w_i)$?</p>
<blockquote>
<p>用语料的频率代替概率（频率学派、条件学派） </p>
<blockquote>
<script type="math/tex; mode=display">P(w_i)=\frac{count(w_i)}{N}</script></blockquote>
</blockquote>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929214832155.png" alt="image-20210929214832155"></p>
<p><strong>语言模型中存在的一些问题</strong></p>
<ul>
<li>当句子中出现语料库中没有出现的词时，会导致句子的概率为0，但是这不意味着这个词（句子）就不会出现在未来的句子中（<em>仅仅只是因为我们的语料库不够充分导致我们误判了该句子出现的概率为0</em>）</li>
<li>当短语过长也可能会导致该短语的出现的概率为0（短语越长越有可能出现这种状况），但是这种短语在未来还是有可能出现的，不应该直接判断为0</li>
</ul>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929225102402.png" alt="image-20210929225102402"></p>
<p><strong>解决办法</strong>：引入平滑操作</p>
<h1 id="统计语言模型中的平滑操作"><a href="#统计语言模型中的平滑操作" class="headerlink" title="统计语言模型中的平滑操作"></a>统计语言模型中的平滑操作</h1><p>有一些词或词组在语料中没有出现过，但是不代表它不可能存在。平滑操作就是给那些没出现过的词或词组一个比较小的概率</p>
<h2 id="1、Laplace-Smoothing-也称Add-one-Smoothing"><a href="#1、Laplace-Smoothing-也称Add-one-Smoothing" class="headerlink" title="1、Laplace Smoothing  也称Add-one Smoothing"></a>1、Laplace Smoothing  也称Add-one Smoothing</h2><blockquote>
<p>Origine: $P(w)=\frac{C(w)}{N}$</p>
<p>Laplace Smoothing:$P(w)=\frac{C(w)+1}{N+V}$</p>
</blockquote>
<p>$C(w)$:表示w出现的次数。<br>$N$:表示词库的维度大小<br>$V$:表示句子中词的个数</p>
<h2 id="2、Add-K-Smoothing"><a href="#2、Add-K-Smoothing" class="headerlink" title="2、Add-K Smoothing"></a>2、Add-K Smoothing</h2><blockquote>
<p>Origine: $P(w)=\frac{C(w)}{N}$</p>
<p>Laplace Smoothing:$P(w)=\frac{C(w)+K}{N+K*V}$</p>
</blockquote>
<h2 id="马尔科夫假设"><a href="#马尔科夫假设" class="headerlink" title="马尔科夫假设"></a>马尔科夫假设</h2><p>下一个词的出现仅依赖于前面的一个词或几个词  </p>
<script type="math/tex; mode=display">𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤_1)P(𝑤_2|𝑤_1)P(𝑤_3|𝑤_1𝑤_2)…P(𝑤_𝑛|𝑤_1𝑤_2 … 𝑤_{𝑛−1})</script><p><strong>unigram</strong>:$𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤<em>1)P(𝑤_2)P(𝑤_3)…P(𝑤</em>𝑛)$</p>
<p><strong>bigram</strong>:$𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤<em>1)P(𝑤_2|𝑤_1)P(𝑤_3|𝑤_2)…P(𝑤</em>𝑛|𝑤_{𝑛−1})$</p>
<p><strong>trigram</strong>:$𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤<em>1)P(𝑤_2|𝑤_1)P(𝑤_3|𝑤_1𝑤_2)…P(𝑤𝑛|𝑤</em>{n-3}𝑤<em>{n-2}𝑤</em>{n-1})$</p>
<p><strong>K-gram</strong>:$𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤<em>1)P(𝑤_2|𝑤_1)P(𝑤_3|𝑤_1𝑤_2)…P(𝑤𝑛|𝑤</em>{n-k}…𝑤_{n-1})$</p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929231331739.png" alt="image-20210929231331739"></p>
<h1 id="语言模型的评价指标：困惑度-Perplexity"><a href="#语言模型的评价指标：困惑度-Perplexity" class="headerlink" title="语言模型的评价指标：困惑度(Perplexity)"></a>语言模型的评价指标：困惑度(Perplexity)</h1><script type="math/tex; mode=display">𝑃(𝑠) = 𝑃(𝑤1, 𝑤2, … , 𝑤𝑛)= 𝑃(𝑤_1)P(𝑤_2|𝑤_1)P(𝑤_3|𝑤_1𝑤_2)…P(𝑤_𝑛|𝑤_1𝑤_2 … 𝑤_{𝑛−1})</script><script type="math/tex; mode=display">PP(s)=P(w_1,w_2,...,w_n)^{-\frac{1}{n}}=\sqrt[N]{\frac{1}{P(w_1,w_2,...,w_n)}}</script><blockquote>
<p>句子概率$P(s)$越大,语言模型越好，困惑度$PP(s)$越小</p>
</blockquote>
<h1 id="词的表示方法"><a href="#词的表示方法" class="headerlink" title="词的表示方法"></a>词的表示方法</h1><h2 id="One-hot编码（独热编码）"><a href="#One-hot编码（独热编码）" class="headerlink" title="One-hot编码（独热编码）"></a>One-hot编码（独热编码）</h2><ul>
<li>优点：表示简单</li>
<li>缺点：<ul>
<li>编码的维度等于词的数量，词越多维度越高，且向量非常稀疏</li>
<li>词语词之间的语义信息无法表示</li>
</ul>
</li>
</ul>
<blockquote>
<p>Example:</p>
<ul>
<li>“话筒”表示为[0 0 0 1 0 0 0 …]</li>
<li>“麦克风”表示为[0 0 0 0 0 1 0 …]</li>
</ul>
<p>$V(话筒)*V(麦克风)=0$我们希望通过向量的点乘表示出向量之间的一定的关系，话筒和麦克风之间显然有一定关系，但是one-hot编码后无法表征出来</p>
<p>稀疏矩阵的表示方法:</p>
<ul>
<li>“话筒”  3:1</li>
<li>“麦克风”  5:1</li>
</ul>
</blockquote>
<h2 id="共生矩阵表示"><a href="#共生矩阵表示" class="headerlink" title="共生矩阵表示"></a>共生矩阵表示</h2><p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929233232531.png" alt="image-20210929233232531"></p>
<h2 id="分布式表示-稠密表示-Distributed-Representation"><a href="#分布式表示-稠密表示-Distributed-Representation" class="headerlink" title="分布式表示/稠密表示(Distributed Representation)"></a>分布式表示/稠密表示(Distributed Representation)</h2><p>分布式表示的概念是深度学习的核心，特别是在它用于NLP任务时。</p>
<p><strong>1、稀疏/本地化的非分布式形状表示（one-hot）</strong></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929234500604.png" alt="&quot;稀疏或叫本地化的非分布式的形状的表示&quot;" title="稀疏或叫本地化的非分布式的形状的表示"></p>
<p>这种非分布式的表示方法在很多方面是效率低下的：</p>
<ul>
<li>随着观察的图形的数量增加，表示的维度增加</li>
<li>没有体现图形之间的相识度</li>
</ul>
<p><strong>2、分布式表示</strong></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929234945667.png" alt="image-20210929234945667"></p>
<p>它用与方向和形状概念相关的多个“记忆单元”表示形状的信息。这些“记忆单元”含有各个形状以及形状和形状之间的关联信息。当我们用分布式表示方法表示一个新的形状时就不用再添加新的维度。</p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210929235402278.png" alt="image-20210929235402278"></p>
<h1 id="对比模型"><a href="#对比模型" class="headerlink" title="对比模型"></a>对比模型</h1><ul>
<li>前馈神经网络语言模型(NNLM)</li>
<li>循环神经网络语言模型(RNNLM)</li>
</ul>
<blockquote>
<p>语言模型是无监督的（不需要标注语料）—-用周围词或之前的词就可以得到词的概率生成样本训练</p>
<p>我们目标是通过无监督的方法进行监督学习</p>
</blockquote>
<p><strong>1、NNLM</strong></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930000326605.png" alt="image-20210930000326605"></p>
<blockquote>
<p>根据前n-1个单词预测第n个位子单词的概率</p>
<p>优化模型，使得输出的正确的单词概率最大化</p>
</blockquote>
<ul>
<li>输入层：将词映射成向量，相当于一个$1<em>V$的one-hot向量乘以一个$V</em>D$的向量得到一个$1*D$的向量</li>
</ul>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930001207016.png" alt="image-20210930001207016"></p>
<blockquote>
<p>相当于用one-hot编码向量来查找自己的词向量(本图例子即为$(w<em>{41},w</em>{42},w_{43})$)</p>
<p>词向量矩阵$V<em>D$其中$D$是特征数，我们自己设置的<em>*超参数</em></em></p>
</blockquote>
<ul>
<li><p>隐藏层：一个以tanh为激活函数的全连接层</p>
<ul>
<li><script type="math/tex; mode=display">a=tanh(d+Ux)</script></li>
</ul>
</li>
<li><p>输出层：一个全连接层，后面接一个softmax函数来生成概率分布</p>
<ul>
<li><script type="math/tex; mode=display">y=b+Wa</script></li>
<li>其中y是一个$1*V$的向量：$P(w<em>t|w</em>{t-n},…,w<em>{t-1})-\frac{exp(y</em>{w_t})}{\sum_i{exp(y_i)}})$</li>
</ul>
</li>
<li><p>Perplexity和Loss</p>
<ul>
<li><script type="math/tex; mode=display">Loss:L=-\frac{1}{T}\sum_{i=1}^{T}logp(w_i|w_{i-n+1},...,w_{i-1})</script></li>
<li><script type="math/tex; mode=display">PP(s)=P(w_1,w_2,...,w_T)^{-\frac{1}{T}}=\sqrt[T]{\frac{1}{P(w_1,w_2,...,w_T)}}</script></li>
<li><script type="math/tex; mode=display">log(PP(s))=-\frac{1}{T}log(P(w_1)P(w_2|w_1),...,P(w_T|w_{T-n+1},...,w_{T-1}))</script><ul>
<li><script type="math/tex; mode=display">log(PP(s))=-\frac{1}{T}(logP(w_1)+logP(w_2|w_1)+...+P(w_T|w_{T-n+1},...,w_{T-1}))</script></li>
<li><script type="math/tex; mode=display">PP(s)=e^L</script></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>2、RNNLM</strong></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930101740093.png" alt="image-20210930101740093"></p>
<hr>
<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p><strong>语言模型思想</strong>：句子中下一个词的出现和前面词是有关系的，所以可以用<strong>前面的词</strong>预测<strong>下一个词</strong>。</p>
<p><strong>Word2Vec思想</strong>：句子中<strong>相近的词</strong>之间是有关系的，Word2Vec的基本思想就是用词来预测词。其中，<strong>Skip-gram</strong>使用中心词预测周围词，<strong>CBOW</strong>使用周围词预测中心词。</p>
<h2 id="Skip-gram-跳字模型"><a href="#Skip-gram-跳字模型" class="headerlink" title="Skip-gram(跳字模型)"></a>Skip-gram(跳字模型)</h2><p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930102330855.png" alt="image-20210930102330855"></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930102714863.png" alt="image-20210930102714863"></p>
<blockquote>
<p>1、将中心词(的index)映射成向量</p>
<p>2、中心词向量乘以<strong>中心词的词向量矩阵</strong>得到特征向量</p>
<p>3、得到的特征向量乘以<strong>周围词的词向量矩阵</strong>得到输出向量</p>
<p>4、输出向量经过softmax函数可以得到周围词的概率分布</p>
</blockquote>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930103303959.png" alt="image-20210930103303959"></p>
<h2 id="CBOW-词袋模型"><a href="#CBOW-词袋模型" class="headerlink" title="CBOW(词袋模型)"></a>CBOW(词袋模型)</h2><p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930103645235.png" alt="image-20210930103645235"></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930103714231.png" alt="image-20210930103714231"></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930103923188.png" alt="image-20210930103923188"></p>
<h2 id="加速softmax"><a href="#加速softmax" class="headerlink" title="加速softmax"></a>加速softmax</h2><p>在进行最优化求解过程中：从隐藏层到输出的Softmax层的计算量很大，因为要计算所有词的Softmax概率，再去找概率最大的值。<br>论文介绍了两种加速Softmax的方法</p>
<ul>
<li>层次Softmax(Hierarchical Softmax)</li>
<li>负采样(Negative Sampling)</li>
</ul>
<h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><ul>
<li>思想：把求softmax转化成求几个sigmoid</li>
</ul>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930104300923.png" alt="image-20210930104300923"></p>
<blockquote>
<p>层次softmax只需要$log_2V$个sigmoid————————————-（V—&gt;$log_2V$）</p>
</blockquote>
<p><strong>Huffman树的构建</strong>：带权重路径最短二叉树(最优二叉树)</p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930104758929.png" alt></p>
<blockquote>
<p>左边是常见的二叉树，右边是左图转换过的最优二叉树。<br><strong>规则</strong>：最重要的放在最前面，由此构成huffman树</p>
</blockquote>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20211005142526795.png" alt="image-20211005142526795"></p>
<p><strong>哈夫曼树编码</strong>：左子树为0，右子树为1。D的编码为0；B的编码为10；C的编码为110；A的编码为111。每个叶子结点代表语料库中的一个词，每个词都可以被01为唯一的编码，并且其编码序列对应一个事件序列</p>
<p><strong>Logistic Regression</strong>的思想就是利用Sigmoid函数把任意只映射到(0,1)的区间上来实现分类问题（主要是二分类问题）。这里我们用Logistic Regression来判断哈夫曼树中走左子树还是右子树，其输出就是走某一条路的概率</p>
<p><strong>CBOW的层次softmax构建</strong></p>
<p>CBOW是已知上下文求中心词的语言模型。</p>
<p>学习目标是最大化对数释然函数：$L=\sum_{w\in C}logP(w|Context(w))$</p>
<p><strong>Skip-gram的层次softmax构建</strong></p>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20210930105038471.png" alt="image-20210930105038471"></p>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>为什么要引入负采样？</p>
<ul>
<li><p>随机梯度下降法存在的问题：</p>
<ul>
<li><p>通过求代价函数的权重梯度，我们可以一次性对所有参数的$\theta$进行优化，但是对于大语料库来说，每次等全部计算完再优化升级我们将等待很长时间。所以我们采用随机梯度下降法（SGD），即每次完成一次计算就进行升级。</p>
</li>
<li><p>我们每次只对窗口中出现的几个单词进行升级，但是在计算梯度的过程中，我们是对整个参数矩阵进行运算，这样参数矩阵中大部分值都是0</p>
<script type="math/tex; mode=display">\nabla_{\theta}J_t(\theta)=\left[\begin{matrix}
0\\
.\\
.\\
.\\
\nabla_{u_{like}}\\
.\\
.\\
.\\
0\\
\nabla_{u_I}\\
.\\
.\\
.\\
\nabla_{u_{learning}}\\
.\\
.\\
.\\
\end{matrix}\right]\in\Bbb{R}^{2dV}</script><blockquote>
<p>计算效率低下</p>
</blockquote>
</li>
<li><p>我们使用的目标函数是softmax函数，分母需要把窗口中所有单词的“得分”都算出来再求和，效率低下</p>
</li>
</ul>
</li>
</ul>
<p><strong>使用负采样</strong></p>
<ul>
<li>核心思想：计算目标单词和窗口单词中单词的真实单词对“得分”，再加一些“噪声”，即词表中的随机单词和目标单词的得分。真实单词对得分和噪声作为代价函数。每次优化参数只关注代价函数中涉及的词向量</li>
</ul>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{T}\sum_{t=1}^TJ_t(\theta)</script><script type="math/tex; mode=display">J_t(\theta)=log\sigma(u_o^Tv_c)+\sum_{i=1}^k\Bbb{E}_jp(w)[log\sigma(-u_j^Tv_c)]</script><blockquote>
<ul>
<li><p>我们仅对k个参数进行了采样</p>
</li>
<li><p>我们放弃了softmax函数，采用了sigmoid函数，这样就不存在先求一遍窗口中所有单词的得分的情况了</p>
</li>
<li><p>其中$v_c$、$v_o$、$v_k$是我们要进行求导更新的目标向量</p>
</li>
</ul>
<p><img src="/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/image-20211007153415343.png" alt="image-20211007153415343"></p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Xu Hao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://xhzzzz.github.io/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/">http://xhzzzz.github.io/2021/09/29/Word2Vec%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://xhZzzz.github.io" target="_blank">界王星</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/10/07/Softmax%E5%92%8CSigmoid%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/"><img class="prev-cover" src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Softmax和Sigmoid笔记总结</div></div></a></div><div class="next-post pull-right"><a href="/2021/09/28/CS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Lecture%2001%20Introduce%20and%20Word%20Vectors/"><img class="next-cover" src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Lecture 01 Introduction and Word Vectors</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://img2.baidu.com/it/u=2642132708,127949625&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Xu Hao</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xhZzzz" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:xxxxxx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%B3%E6%BB%91%E6%93%8D%E4%BD%9C"><span class="toc-text">统计语言模型中的平滑操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81Laplace-Smoothing-%E4%B9%9F%E7%A7%B0Add-one-Smoothing"><span class="toc-text">1、Laplace Smoothing  也称Add-one Smoothing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81Add-K-Smoothing"><span class="toc-text">2、Add-K Smoothing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%81%87%E8%AE%BE"><span class="toc-text">马尔科夫假设</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%EF%BC%9A%E5%9B%B0%E6%83%91%E5%BA%A6-Perplexity"><span class="toc-text">语言模型的评价指标：困惑度(Perplexity)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%8D%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95"><span class="toc-text">词的表示方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#One-hot%E7%BC%96%E7%A0%81%EF%BC%88%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81%EF%BC%89"><span class="toc-text">One-hot编码（独热编码）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B1%E7%94%9F%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA"><span class="toc-text">共生矩阵表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA-%E7%A8%A0%E5%AF%86%E8%A1%A8%E7%A4%BA-Distributed-Representation"><span class="toc-text">分布式表示&#x2F;稠密表示(Distributed Representation)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E6%A8%A1%E5%9E%8B"><span class="toc-text">对比模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Word2Vec"><span class="toc-text">Word2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Skip-gram-%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B"><span class="toc-text">Skip-gram(跳字模型)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CBOW-%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">CBOW(词袋模型)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A0%E9%80%9Fsoftmax"><span class="toc-text">加速softmax</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hierarchical-Softmax"><span class="toc-text">Hierarchical Softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Negative-Sampling"><span class="toc-text">Negative Sampling</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/11/16/KGEModel%E7%B3%BB%E5%88%97%E4%B9%8BRotatE/" title="KGEModel系列之RotatE"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="KGEModel系列之RotatE"/></a><div class="content"><a class="title" href="/2021/11/16/KGEModel%E7%B3%BB%E5%88%97%E4%B9%8BRotatE/" title="KGEModel系列之RotatE">KGEModel系列之RotatE</a><time datetime="2021-11-16T13:55:43.000Z" title="发表于 2021-11-16 21:55:43">2021-11-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/11/08/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9F%A5%E8%AF%86%E8%A1%A8%E8%BE%BE%E4%B9%8BTransE%E6%A8%A1%E5%9E%8B/" title="知识图谱知识表达之TransE模型"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="知识图谱知识表达之TransE模型"/></a><div class="content"><a class="title" href="/2021/11/08/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9F%A5%E8%AF%86%E8%A1%A8%E8%BE%BE%E4%B9%8BTransE%E6%A8%A1%E5%9E%8B/" title="知识图谱知识表达之TransE模型">知识图谱知识表达之TransE模型</a><time datetime="2021-11-08T12:28:38.000Z" title="发表于 2021-11-08 20:28:38">2021-11-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/31/NLP%E5%B8%B8%E7%94%A8metrics%E6%95%B4%E7%90%86/" title="NLP常用metrics整理"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NLP常用metrics整理"/></a><div class="content"><a class="title" href="/2021/10/31/NLP%E5%B8%B8%E7%94%A8metrics%E6%95%B4%E7%90%86/" title="NLP常用metrics整理">NLP常用metrics整理</a><time datetime="2021-10-31T02:47:06.000Z" title="发表于 2021-10-31 10:47:06">2021-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/29/Transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="Transformer论文阅读笔记"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer论文阅读笔记"/></a><div class="content"><a class="title" href="/2021/10/29/Transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="Transformer论文阅读笔记">Transformer论文阅读笔记</a><time datetime="2021-10-29T15:15:14.000Z" title="发表于 2021-10-29 23:15:14">2021-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/28/Building-a-Large-scale-Multimodal-Knowledge-Base-System-for-Answering-Visual-Queries/" title="Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries"><img src="https://img2.baidu.com/it/u=196582304,1580603520&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries"/></a><div class="content"><a class="title" href="/2021/10/28/Building-a-Large-scale-Multimodal-Knowledge-Base-System-for-Answering-Visual-Queries/" title="Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries">Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries</a><time datetime="2021-10-28T13:41:22.000Z" title="发表于 2021-10-28 21:41:22">2021-10-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Xu Hao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>